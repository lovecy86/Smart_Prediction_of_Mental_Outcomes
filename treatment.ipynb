{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H95RP3zN4xs_",
        "outputId": "c4843b64-e928-4d25-b6bd-b625ab525643"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to r2u.stat.illinois.\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to r2u.stat.illinois.\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r                                                                               \rHit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Find the latest version of spark 3.x  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-3.5.5'\n",
        "spark_version = 'spark-3.5.5'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulPPOPx76XQo"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "from pyspark.sql import SparkSession\n",
        "import time\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"Mental Health Analysis\").getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtIjOqU-NRYO",
        "outputId": "42d07046-b9ef-4f8c-879b-13b43f602aa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+------+-------------+----------+-------------+--------------+---------+------------+--------------+--------------+---------------------+-----------+----------------+-------------+---------------+-----------------------+------------+\n",
            "|          Timestamp|Gender|      Country|Occupation|self_employed|family_history|treatment|Days_Indoors|Growing_Stress|Changes_Habits|Mental_Health_History|Mood_Swings|Coping_Struggles|Work_Interest|Social_Weakness|mental_health_interview|care_options|\n",
            "+-------------------+------+-------------+----------+-------------+--------------+---------+------------+--------------+--------------+---------------------+-----------+----------------+-------------+---------------+-----------------------+------------+\n",
            "|2014-08-27 11:29:31|Female|United States| Corporate|         NULL|            No|      Yes|   1-14 days|           Yes|            No|                  Yes|     Medium|              No|           No|            Yes|                     No|    Not sure|\n",
            "|2014-08-27 11:31:50|Female|United States| Corporate|         NULL|           Yes|      Yes|   1-14 days|           Yes|            No|                  Yes|     Medium|              No|           No|            Yes|                     No|          No|\n",
            "|2014-08-27 11:32:39|Female|United States| Corporate|         NULL|           Yes|      Yes|   1-14 days|           Yes|            No|                  Yes|     Medium|              No|           No|            Yes|                     No|         Yes|\n",
            "|2014-08-27 11:37:59|Female|United States| Corporate|           No|           Yes|      Yes|   1-14 days|           Yes|            No|                  Yes|     Medium|              No|           No|            Yes|                  Maybe|         Yes|\n",
            "|2014-08-27 11:43:36|Female|United States| Corporate|           No|           Yes|      Yes|   1-14 days|           Yes|            No|                  Yes|     Medium|              No|           No|            Yes|                     No|         Yes|\n",
            "+-------------------+------+-------------+----------+-------------+--------------+---------+------------+--------------+--------------+---------------------+-----------+----------------+-------------+---------------+-----------------------+------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- Timestamp: timestamp (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            " |-- Occupation: string (nullable = true)\n",
            " |-- self_employed: string (nullable = true)\n",
            " |-- family_history: string (nullable = true)\n",
            " |-- treatment: string (nullable = true)\n",
            " |-- Days_Indoors: string (nullable = true)\n",
            " |-- Growing_Stress: string (nullable = true)\n",
            " |-- Changes_Habits: string (nullable = true)\n",
            " |-- Mental_Health_History: string (nullable = true)\n",
            " |-- Mood_Swings: string (nullable = true)\n",
            " |-- Coping_Struggles: string (nullable = true)\n",
            " |-- Work_Interest: string (nullable = true)\n",
            " |-- Social_Weakness: string (nullable = true)\n",
            " |-- mental_health_interview: string (nullable = true)\n",
            " |-- care_options: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "csv_path = \"mental_health_dataset/Mental Health Dataset.csv\"  # Adjust filename if different\n",
        "\n",
        "df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
        "df.show(5)\n",
        "df.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJK7tNVoWNcj"
      },
      "source": [
        "# **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vupES8IaWpnq"
      },
      "source": [
        "Drop Timestamp - Not useful for modeling treatment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxDUyY-VPRyS",
        "outputId": "8b54cbb6-e838-483b-db3f-b71fed1a9620"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total rows: 292364\n",
            "+-------+------+-------------+----------+-------------+--------------+---------+------------------+--------------+--------------+---------------------+-----------+----------------+-------------+---------------+-----------------------+------------+\n",
            "|summary|Gender|      Country|Occupation|self_employed|family_history|treatment|      Days_Indoors|Growing_Stress|Changes_Habits|Mental_Health_History|Mood_Swings|Coping_Struggles|Work_Interest|Social_Weakness|mental_health_interview|care_options|\n",
            "+-------+------+-------------+----------+-------------+--------------+---------+------------------+--------------+--------------+---------------------+-----------+----------------+-------------+---------------+-----------------------+------------+\n",
            "|  count|292364|       292364|    292364|       287162|        292364|   292364|            292364|        292364|        292364|               292364|     292364|          292364|       292364|         292364|                 292364|      292364|\n",
            "|   mean|  NULL|         NULL|      NULL|         NULL|          NULL|     NULL|              NULL|          NULL|          NULL|                 NULL|       NULL|            NULL|         NULL|           NULL|                   NULL|        NULL|\n",
            "| stddev|  NULL|         NULL|      NULL|         NULL|          NULL|     NULL|              NULL|          NULL|          NULL|                 NULL|       NULL|            NULL|         NULL|           NULL|                   NULL|        NULL|\n",
            "|    min|Female|    Australia|  Business|           No|            No|       No|         1-14 days|         Maybe|         Maybe|                Maybe|       High|              No|        Maybe|          Maybe|                  Maybe|          No|\n",
            "|    25%|  NULL|         NULL|      NULL|         NULL|          NULL|     NULL|              NULL|          NULL|          NULL|                 NULL|       NULL|            NULL|         NULL|           NULL|                   NULL|        NULL|\n",
            "|    50%|  NULL|         NULL|      NULL|         NULL|          NULL|     NULL|              NULL|          NULL|          NULL|                 NULL|       NULL|            NULL|         NULL|           NULL|                   NULL|        NULL|\n",
            "|    75%|  NULL|         NULL|      NULL|         NULL|          NULL|     NULL|              NULL|          NULL|          NULL|                 NULL|       NULL|            NULL|         NULL|           NULL|                   NULL|        NULL|\n",
            "|    max|  Male|United States|   Student|          Yes|           Yes|      Yes|More than 2 months|           Yes|           Yes|                  Yes|     Medium|             Yes|          Yes|            Yes|                    Yes|         Yes|\n",
            "+-------+------+-------------+----------+-------------+--------------+---------+------------------+--------------+--------------+---------------------+-----------+----------------+-------------+---------------+-----------------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Count the number of rows\n",
        "print(f\"Total rows: {df.count()}\")\n",
        "\n",
        "# Check for missing values in key columns\n",
        "df.summary().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIr68aJYQhgq",
        "outputId": "da7379fa-9efa-42e9-dff8-202fec51e9ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+------+-------+----------+-------------+--------------+---------+------------+--------------+--------------+---------------------+-----------+----------------+-------------+---------------+-----------------------+------------+\n",
            "|Timestamp|Gender|Country|Occupation|self_employed|family_history|treatment|Days_Indoors|Growing_Stress|Changes_Habits|Mental_Health_History|Mood_Swings|Coping_Struggles|Work_Interest|Social_Weakness|mental_health_interview|care_options|\n",
            "+---------+------+-------+----------+-------------+--------------+---------+------------+--------------+--------------+---------------------+-----------+----------------+-------------+---------------+-----------------------+------------+\n",
            "|        0|     0|      0|         0|         5202|             0|        0|           0|             0|             0|                    0|          0|               0|            0|              0|                      0|           0|\n",
            "+---------+------+-------+----------+-------------+--------------+---------+------------+--------------+--------------+---------------------+-----------+----------------+-------------+---------------+-----------------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import isnull, count, when\n",
        "\n",
        "# Check for missing values in each column\n",
        "df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAYArlCMQd05",
        "outputId": "1032991e-5fb5-41e5-a735-a2cbae550d90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "287162"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = df.dropna(subset=['self_employed'])\n",
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIG204gtWXnm",
        "outputId": "e6de1e53-dd90-495c-849f-a945f9ce7b12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "286808"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# from pyspark.sql.functions import count, col\n",
        "\n",
        "# # Group by all columns and count occurrences\n",
        "# duplicate_counts = df.groupBy(df.columns).agg(count(\"*\").alias(\"count\"))\n",
        "\n",
        "# # Filter for rows with count > 1 (duplicates)\n",
        "# duplicate_rows = duplicate_counts.filter(col(\"count\") > 1)\n",
        "\n",
        "# # Show the duplicate rows and their counts\n",
        "# duplicate_rows.show()\n",
        "df = df.dropDuplicates()\n",
        "df.count()\n",
        "\n",
        "# Optionally, you can drop the \"count\" column if you only need the duplicate rows:\n",
        "# duplicate_rows = duplicate_rows.drop(\"count\")\n",
        "# duplicate_rows.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXUByFExWdAF",
        "outputId": "4e8897b6-cf59-4157-8daa-87cc532841c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "286808"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = df.drop(\"Timestamp\")\n",
        "df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5eD17SyVsbd"
      },
      "source": [
        "# **1. Using LOGISTIC REGRESSION MODEL**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdnQOG8FanpM"
      },
      "source": [
        "# **Define Target and Categorical Columns**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LOGISTIC REGRESSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaoIfwxeavm9"
      },
      "outputs": [],
      "source": [
        "target_col = \"treatment\"\n",
        "categorical_cols = [col for col in df.columns if col != target_col]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quKaQJySa_St"
      },
      "source": [
        "# **Encode Categorical Variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGJ0z4rtavgY"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "\n",
        "# Create StringIndexer for each categorical column\n",
        "indexers = [\n",
        "    StringIndexer(inputCol=column, outputCol=column + \"_index\", handleInvalid='keep')\n",
        "    for column in categorical_cols + [target_col]\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZdyGdtmbhjg"
      },
      "source": [
        "# **Assemble Features into One Vector**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38R1oa-KavcR"
      },
      "outputs": [],
      "source": [
        "assembler = VectorAssembler(\n",
        "    inputCols=[col + \"_index\" for col in categorical_cols],\n",
        "    outputCol=\"features\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKajim-Sb7us"
      },
      "source": [
        "# **1. Define the Logistic Regression Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYJDkvq2avXd"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "lr = LogisticRegression(labelCol=\"treatment_index\", featuresCol=\"features\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOWTTkAFcTUs"
      },
      "source": [
        "# **Build a Pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwDvth-savSx"
      },
      "outputs": [],
      "source": [
        "pipeline = Pipeline(stages=indexers + [assembler, lr])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT_2AiIMcbm_"
      },
      "source": [
        "# **Split the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4mYjuE-avNm"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lFnH0UrcknT"
      },
      "source": [
        "# **Train the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-MVqvyScokF"
      },
      "outputs": [],
      "source": [
        "model = pipeline.fit(train_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6rhW46Lde19"
      },
      "source": [
        "# **Make Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NISCoH9cdkDh"
      },
      "outputs": [],
      "source": [
        "predictions = model.transform(test_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKRYi4LNdo6v"
      },
      "source": [
        "# **Evaluate Accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMKAAc5teB_8",
        "outputId": "51f3bfc9-2440-49ed-8f10-6aec0cdc1913"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+\n",
            "|treatment_index|\n",
            "+---------------+\n",
            "|            0.0|\n",
            "|            1.0|\n",
            "+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "predictions.select(\"treatment_index\").distinct().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y68qC8avfaX0"
      },
      "source": [
        "# **Evaluate Accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otDqkhgWew4m"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Use 'areaUnderROC' or 'areaUnderPR' for raw predictions\n",
        "evaluator_acc = BinaryClassificationEvaluator(\n",
        "    labelCol=\"treatment_index\",\n",
        "    rawPredictionCol=\"prediction\",  # Using rawPredictionCol\n",
        "    metricName=\"areaUnderROC\"  # Changed metricName to 'areaUnderROC'\n",
        ")\n",
        "\n",
        "accuracy = evaluator_acc.evaluate(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2r6VsKJfVAa"
      },
      "source": [
        "# **R2 Score**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qka8eBdce-0C"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator # Import RegressionEvaluator\n",
        "\n",
        "r2_evaluator = RegressionEvaluator(\n",
        "    labelCol=\"treatment_index\", predictionCol=\"prediction\", metricName=\"r2\"\n",
        ")\n",
        "r2_score = r2_evaluator.evaluate(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPzSK-aofMIx"
      },
      "source": [
        "# **Final Results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fA7wYi8MfEMs",
        "outputId": "813cf0a6-bf6a-4d25-dc9d-ce25b1475f73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Accuracy (AUC): 0.6816\n",
            "R2 Score: -0.2755\n"
          ]
        }
      ],
      "source": [
        "print(f\"Logistic Regression Accuracy (AUC): {accuracy:.4f}\")\n",
        "print(f\"R2 Score: {r2_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA2PHMPio8_7"
      },
      "source": [
        "# **2. Using RANDOM FOREST MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OO2ttFKvpEFE",
        "outputId": "7d184af2-2bcd-4c01-a969-f1172c7ca922"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7206\n",
            "R2 Score: 0.2144\n",
            "AUC: 0.7922\n"
          ]
        }
      ],
      "source": [
        "#RANDOM fOREST\n",
        "#  4. Convert target 'treatment' column to numerical\n",
        "from pyspark.ml.feature import OneHotEncoder\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.functions import vector_to_array\n",
        "from pyspark.sql.functions import col\n",
        "label_indexer = StringIndexer(inputCol=\"treatment\", outputCol=\"label\")\n",
        "\n",
        "# 5. Identify categorical features\n",
        "categorical_cols = [col for col in df.columns if col != \"treatment\"]\n",
        "\n",
        "# Index categorical columns\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_Index\", handleInvalid='keep') for col in categorical_cols]\n",
        "\n",
        "# Prepare encoded column names\n",
        "indexed_cols = [col + \"_Index\" for col in categorical_cols]\n",
        "encoded_cols = [col + \"_Vec\" for col in indexed_cols]\n",
        "\n",
        "# One-hot encode\n",
        "encoder = OneHotEncoder(inputCols=indexed_cols, outputCols=encoded_cols)\n",
        "\n",
        "# Assemble all features into one vector\n",
        "assembler = VectorAssembler(inputCols=encoded_cols, outputCol=\"features\")\n",
        "\n",
        "# 8. Random Forest Classifier\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=100)\n",
        "\n",
        "# 9. Build pipeline\n",
        "pipeline = Pipeline(stages=indexers + [encoder, assembler, label_indexer, rf])\n",
        "\n",
        "# 10. Split data\n",
        "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# 11. Train the model\n",
        "model = pipeline.fit(train_data)\n",
        "\n",
        "# 12. Make predictions\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# 13. Evaluate Accuracy\n",
        "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = accuracy_evaluator.evaluate(predictions)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# 14. Evaluate R2 Score using Binary Classification Evaluator (AUC is better, but for R2 approximation...)\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Convert vector to array and extract probability of class 1\n",
        "preds_with_prob = predictions.withColumn(\"probability_array\", vector_to_array(\"probability\"))\n",
        "preds_with_prob = preds_with_prob.withColumn(\"prob_class_1\", col(\"probability_array\")[1])\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "# UDF to extract the second element (probability of class 1)\n",
        "get_prob = udf(lambda v: float(v[1]), DoubleType())\n",
        "\n",
        "# Apply it to extract the predicted probability for class 1\n",
        "preds_with_prob = predictions.withColumn(\"prob_class_1\", get_prob(predictions[\"probability\"]))\n",
        "\n",
        "reg_eval = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prob_class_1\", metricName=\"r2\")\n",
        "r2 = reg_eval.evaluate(preds_with_prob)\n",
        "print(f\"R2 Score: {r2:.4f}\")\n",
        "\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "auc_evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "auc = auc_evaluator.evaluate(predictions)\n",
        "print(f\"AUC: {auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O_sYrLeVXsG"
      },
      "source": [
        "# **3. Using CATBOOST MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6ho9be3brQG",
        "outputId": "ee53a3ee-f4cc-48ea-fd17-54ca4c300410"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Collecting numpy<2.0,>=1.16.0 (from catboost)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.14.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, catboost\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed catboost-1.2.7 numpy-1.26.4\n"
          ]
        }
      ],
      "source": [
        "# CATBOOST\n",
        "pip install catboost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKVyP_GGd-Qq",
        "outputId": "0ddefe42-38e8-4bc5-b052-3900d9bf68f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numpy\n",
            "  Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "catboost 1.2.7 requires numpy<2.0,>=1.16.0, but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.4\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --force-reinstall numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Gey90xENeP_Z",
        "outputId": "4a24fd0e-4a7a-4517-f39c-3d9dafa436bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting catboost\n",
            "  Using cached catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting graphviz (from catboost)\n",
            "  Downloading graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting matplotlib (from catboost)\n",
            "  Downloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting numpy<2.0,>=1.16.0 (from catboost)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting pandas>=0.24 (from catboost)\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy (from catboost)\n",
            "  Downloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting plotly (from catboost)\n",
            "  Downloading plotly-6.0.1-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting six (from catboost)\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas>=0.24->catboost)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas>=0.24->catboost)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas>=0.24->catboost)\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib->catboost)\n",
            "  Downloading contourpy-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib->catboost)\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib->catboost)\n",
            "  Downloading fonttools-4.57.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (102 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.5/102.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib->catboost)\n",
            "  Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
            "Collecting packaging>=20.0 (from matplotlib->catboost)\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting pillow>=8 (from matplotlib->catboost)\n",
            "  Downloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib->catboost)\n",
            "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting narwhals>=1.15.1 (from plotly->catboost)\n",
            "  Downloading narwhals-1.34.1-py3-none-any.whl.metadata (9.2 kB)\n",
            "Using cached catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl (98.7 MB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading plotly-6.0.1-py3-none-any.whl (14.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading contourpy-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.2/326.2 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.57.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading narwhals-1.34.1-py3-none-any.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.5/325.5 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytz, tzdata, six, pyparsing, pillow, packaging, numpy, narwhals, kiwisolver, graphviz, fonttools, cycler, scipy, python-dateutil, plotly, contourpy, pandas, matplotlib, catboost\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2025.2\n",
            "    Uninstalling pytz-2025.2:\n",
            "      Successfully uninstalled pytz-2025.2\n",
            "  Attempting uninstall: tzdata\n",
            "    Found existing installation: tzdata 2025.2\n",
            "    Uninstalling tzdata-2025.2:\n",
            "      Successfully uninstalled tzdata-2025.2\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.2.3\n",
            "    Uninstalling pyparsing-3.2.3:\n",
            "      Successfully uninstalled pyparsing-3.2.3\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.1.0\n",
            "    Uninstalling pillow-11.1.0:\n",
            "      Successfully uninstalled pillow-11.1.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.2\n",
            "    Uninstalling packaging-24.2:\n",
            "      Successfully uninstalled packaging-24.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.4\n",
            "    Uninstalling numpy-2.2.4:\n",
            "      Successfully uninstalled numpy-2.2.4\n",
            "  Attempting uninstall: narwhals\n",
            "    Found existing installation: narwhals 1.33.0\n",
            "    Uninstalling narwhals-1.33.0:\n",
            "      Successfully uninstalled narwhals-1.33.0\n",
            "  Attempting uninstall: kiwisolver\n",
            "    Found existing installation: kiwisolver 1.4.8\n",
            "    Uninstalling kiwisolver-1.4.8:\n",
            "      Successfully uninstalled kiwisolver-1.4.8\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.20.3\n",
            "    Uninstalling graphviz-0.20.3:\n",
            "      Successfully uninstalled graphviz-0.20.3\n",
            "  Attempting uninstall: fonttools\n",
            "    Found existing installation: fonttools 4.57.0\n",
            "    Uninstalling fonttools-4.57.0:\n",
            "      Successfully uninstalled fonttools-4.57.0\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.12.1\n",
            "    Uninstalling cycler-0.12.1:\n",
            "      Successfully uninstalled cycler-0.12.1\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.2\n",
            "    Uninstalling python-dateutil-2.8.2:\n",
            "      Successfully uninstalled python-dateutil-2.8.2\n",
            "  Attempting uninstall: plotly\n",
            "    Found existing installation: plotly 5.24.1\n",
            "    Uninstalling plotly-5.24.1:\n",
            "      Successfully uninstalled plotly-5.24.1\n",
            "  Attempting uninstall: contourpy\n",
            "    Found existing installation: contourpy 1.3.1\n",
            "    Uninstalling contourpy-1.3.1:\n",
            "      Successfully uninstalled contourpy-1.3.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "  Attempting uninstall: catboost\n",
            "    Found existing installation: catboost 1.2.7\n",
            "    Uninstalling catboost-1.2.7:\n",
            "      Successfully uninstalled catboost-1.2.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed catboost-1.2.7 contourpy-1.3.1 cycler-0.12.1 fonttools-4.57.0 graphviz-0.20.3 kiwisolver-1.4.8 matplotlib-3.10.1 narwhals-1.34.1 numpy-1.26.4 packaging-24.2 pandas-2.2.3 pillow-11.2.1 plotly-6.0.1 pyparsing-3.2.3 python-dateutil-2.9.0.post0 pytz-2025.2 scipy-1.15.2 six-1.17.0 tzdata-2025.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "4d8d7e494423431ea55fa107c2e96f86",
              "pip_warning": {
                "packages": [
                  "PIL",
                  "_catboost",
                  "cycler",
                  "dateutil",
                  "kiwisolver",
                  "pytz",
                  "six"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install --upgrade --force-reinstall catboost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OD-l5x1ipEBM",
        "outputId": "4f9d20a8-1527-45d2-9c61-2777b50407ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/spark-3.5.5-bin-hadoop3/python/pyspark/sql/pandas/functions.py:407: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
            "  warnings.warn(\n",
            "/content/spark-3.5.5-bin-hadoop3/python/pyspark/sql/pandas/functions.py:407: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.3337633538813422\n",
            "+---------+----------+\n",
            "|treatment|prediction|\n",
            "+---------+----------+\n",
            "|      Yes|       Yes|\n",
            "|      Yes|       Yes|\n",
            "|      Yes|       Yes|\n",
            "|      Yes|       Yes|\n",
            "|       No|       Yes|\n",
            "|      Yes|       Yes|\n",
            "|      Yes|       Yes|\n",
            "|      Yes|       Yes|\n",
            "|      Yes|       Yes|\n",
            "|       No|       Yes|\n",
            "+---------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import pandas_udf, PandasUDFType, col as pyspark_col, when\n",
        "from pyspark.sql.types import StringType  # Change to StringType\n",
        "import catboost as cb\n",
        "import numpy as np\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "import pandas as pd # Import pandas and alias it as 'pd'\n",
        "\n",
        "categorical_columns = ['Gender', 'Country', 'Occupation', 'self_employed', 'family_history',\n",
        "                       'Days_Indoors', 'Growing_Stress', 'Changes_Habits', 'Mental_Health_History',\n",
        "                       'Mood_Swings', 'Coping_Struggles', 'Work_Interest', 'Social_Weakness',\n",
        "                       'mental_health_interview', 'care_options']\n",
        "\n",
        "# Define the target and feature columns\n",
        "target_col = \"treatment\"\n",
        "feature_cols = [col for col in categorical_columns if col != target_col]\n",
        "\n",
        "# Index the categorical features and label\n",
        "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_Index\", handleInvalid=\"keep\") for column in feature_cols]\n",
        "label_indexer = StringIndexer(inputCol=target_col, outputCol=\"label\", handleInvalid=\"keep\")\n",
        "\n",
        "# Create the pipeline for indexing\n",
        "indexing_pipeline = Pipeline(stages=indexers + [label_indexer])\n",
        "indexed_df = indexing_pipeline.fit(df).transform(df)\n",
        "\n",
        "# Convert Spark DataFrame to Pandas DataFrame for CatBoost\n",
        "pandas_df = indexed_df.select([target_col, *[col+\"_Index\" for col in feature_cols]]).toPandas()\n",
        "\n",
        "# Cast all columns in the Pandas dataframe to int\n",
        "# This is necessary to ensure CatBoost interprets them as categorical features\n",
        "for col in pandas_df.columns:\n",
        "    if col != target_col:  # Exclude target column\n",
        "        pandas_df[col] = pandas_df[col].astype(int)\n",
        "\n",
        "# Define CatBoost model\n",
        "catboost_model = cb.CatBoostClassifier(iterations=100,  # Adjust parameters as needed\n",
        "                                      learning_rate=0.1,\n",
        "                                      depth=6,\n",
        "                                      loss_function='MultiClass',\n",
        "                                      verbose=False)\n",
        "\n",
        "# Define features and target for CatBoost\n",
        "X = pandas_df[[col+\"_Index\" for col in feature_cols]]\n",
        "y = pandas_df[target_col]\n",
        "\n",
        "# Fit the CatBoost model\n",
        "catboost_model.fit(X, y, cat_features=list(X.columns))  # Specify categorical features\n",
        "# assembler = VectorAssembler(inputCols = [col+\"_Index\" for col in feature_cols], outputCol = \"features\")\n",
        "# assembled_df = assembler.transform(indexed_df)\n",
        "# Create a Pandas UDF for prediction\n",
        "\n",
        "# Create a Pandas UDF for prediction\n",
        "@pandas_udf(returnType=StringType(), functionType=PandasUDFType.SCALAR)  # Change to StringType\n",
        "def predict_udf(*cols) -> pd.Series:\n",
        "    features = pd.DataFrame(list(zip(*cols)), columns=[col+\"_Index\" for col in feature_cols])\n",
        "    for col in features.columns:\n",
        "        features[col] = features[col].astype(int)\n",
        "    predictions = catboost_model.predict(features)\n",
        "    return pd.Series(predictions.flatten())\n",
        "\n",
        "indexed_df = indexed_df.withColumn(\"prediction\", predict_udf(\n",
        "    *[indexed_df[col+\"_Index\"] for col in feature_cols]\n",
        "))\n",
        "\n",
        "from pyspark.sql.functions import pandas_udf, PandasUDFType, col, when\n",
        "from pyspark.sql.types import StringType  # Change to StringType\n",
        "import catboost as cb\n",
        "import numpy as np\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "import pandas as pd # Import pandas and alias it as 'pd'\n",
        "\n",
        "# Define the target and feature columns\n",
        "target_col = \"treatment\"\n",
        "feature_cols = [col for col in categorical_columns if col != target_col]\n",
        "\n",
        "# Index the categorical features and label\n",
        "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_Index\", handleInvalid=\"keep\") for column in feature_cols]\n",
        "label_indexer = StringIndexer(inputCol=target_col, outputCol=\"label\", handleInvalid=\"keep\")\n",
        "\n",
        "# Create the pipeline for indexing\n",
        "indexing_pipeline = Pipeline(stages=indexers + [label_indexer])\n",
        "indexed_df = indexing_pipeline.fit(df).transform(df)\n",
        "\n",
        "# Convert Spark DataFrame to Pandas DataFrame for CatBoost\n",
        "pandas_df = indexed_df.select([target_col, *[col+\"_Index\" for col in feature_cols]]).toPandas()\n",
        "\n",
        "# Cast all columns in the Pandas dataframe to int\n",
        "# This is necessary to ensure CatBoost interprets them as categorical features\n",
        "for col in pandas_df.columns:\n",
        "    if col != target_col:  # Exclude target column\n",
        "        pandas_df[col] = pandas_df[col].astype(int)\n",
        "\n",
        "# Define CatBoost model\n",
        "catboost_model = cb.CatBoostClassifier(iterations=100,  # Adjust parameters as needed\n",
        "                                      learning_rate=0.1,\n",
        "                                      depth=6,\n",
        "                                      loss_function='MultiClass',\n",
        "                                      verbose=False)\n",
        "\n",
        "# Define features and target for CatBoost\n",
        "X = pandas_df[[col+\"_Index\" for col in feature_cols]]\n",
        "y = pandas_df[target_col]\n",
        "\n",
        "# Fit the CatBoost model\n",
        "catboost_model.fit(X, y, cat_features=list(X.columns))  # Specify categorical features\n",
        "# assembler = VectorAssembler(inputCols = [col+\"_Index\" for col in feature_cols], outputCol = \"features\")\n",
        "# assembled_df = assembler.transform(indexed_df)\n",
        "# Create a Pandas UDF for prediction\n",
        "\n",
        "# Create a Pandas UDF for prediction\n",
        "@pandas_udf(returnType=StringType(), functionType=PandasUDFType.SCALAR)  # Change to StringType\n",
        "def predict_udf(*cols) -> pd.Series:\n",
        "    features = pd.DataFrame(list(zip(*cols)), columns=[col+\"_Index\" for col in feature_cols])\n",
        "    for col in features.columns:\n",
        "        features[col] = features[col].astype(int)\n",
        "    predictions = catboost_model.predict(features)\n",
        "    return pd.Series(predictions.flatten()).astype(str)\n",
        "\n",
        "indexed_df = indexed_df.withColumn(\"prediction\", predict_udf(\n",
        "    *[indexed_df[col+\"_Index\"] for col in feature_cols]\n",
        "))\n",
        "\n",
        "\n",
        "# # Evaluate the model (you might need to adjust this part based on your evaluation metrics)\n",
        "accuracy = indexed_df.filter(pyspark_col(\"treatment\").isNotNull() & pyspark_col(\"prediction\").isNotNull()).withColumn(\"correct\", when(pyspark_col(\"Growing_Stress\") == pyspark_col(\"prediction\"), 1).otherwise(0)).selectExpr(\"avg(correct) as accuracy\").first()[\"accuracy\"]\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "# # Show some predictions\n",
        "indexed_df.select(\"treatment\", \"prediction\").show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUTDgIMwhxMG"
      },
      "source": [
        "# **4. Using XGBOOST Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQtZ8ut5pD9T",
        "outputId": "3754cbb6-328d-44d0-8aa6-94a59907b3d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [19:44:59] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/content/spark-3.5.5-bin-hadoop3/python/pyspark/sql/pandas/functions.py:407: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.2163\n",
            "+---------+----------+\n",
            "|treatment|prediction|\n",
            "+---------+----------+\n",
            "|      Yes|         1|\n",
            "|      Yes|         1|\n",
            "|      Yes|         1|\n",
            "|      Yes|         1|\n",
            "|       No|         1|\n",
            "|      Yes|         1|\n",
            "|      Yes|         1|\n",
            "|      Yes|         1|\n",
            "|      Yes|         1|\n",
            "|       No|         1|\n",
            "+---------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# XGBOOST from pyspark.sql.functions import pandas_udf, PandasUDFType, col as pyspark_col, when\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "\n",
        "# Define categorical columns\n",
        "categorical_columns = ['Gender', 'Country', 'Occupation', 'self_employed', 'family_history',\n",
        "                       'Days_Indoors', 'Growing_Stress', 'Changes_Habits', 'Mental_Health_History',\n",
        "                       'Mood_Swings', 'Coping_Struggles', 'Work_Interest', 'Social_Weakness',\n",
        "                       'mental_health_interview', 'care_options']\n",
        "\n",
        "# Define target and features\n",
        "target_col = \"treatment\"\n",
        "feature_cols = [col for col in categorical_columns if col != target_col]\n",
        "\n",
        "# Index categorical columns\n",
        "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_Index\", handleInvalid=\"keep\") for column in feature_cols]\n",
        "label_indexer = StringIndexer(inputCol=target_col, outputCol=\"label\", handleInvalid=\"keep\")\n",
        "\n",
        "# Create pipeline\n",
        "indexing_pipeline = Pipeline(stages=indexers + [label_indexer])\n",
        "indexed_df = indexing_pipeline.fit(df).transform(df)\n",
        "\n",
        "# Convert to pandas\n",
        "pandas_df = indexed_df.select([target_col, *[col+\"_Index\" for col in feature_cols]]).toPandas()\n",
        "\n",
        "# Ensure all features are numeric\n",
        "for col in pandas_df.columns:\n",
        "    if col != target_col:\n",
        "        pandas_df[col] = pandas_df[col].astype(int)\n",
        "\n",
        "# Prepare X and y\n",
        "X = pandas_df[[col+\"_Index\" for col in feature_cols]]\n",
        "y = pandas_df[target_col].astype('category').cat.codes  # Convert target to numeric if not already\n",
        "\n",
        "# Train XGBoost model\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, use_label_encoder=False, eval_metric='mlogloss')\n",
        "xgb_model.fit(X, y)\n",
        "\n",
        "# Predict function using pandas_udf\n",
        "@pandas_udf(returnType=StringType(), functionType=PandasUDFType.SCALAR)\n",
        "def predict_udf(*cols) -> pd.Series:\n",
        "    features = pd.DataFrame(list(zip(*cols)), columns=[col+\"_Index\" for col in feature_cols])\n",
        "    for col in features.columns:\n",
        "        features[col] = features[col].astype(int)\n",
        "    predictions = xgb_model.predict(features)\n",
        "    return pd.Series(predictions.astype(str))\n",
        "\n",
        "# Apply prediction to Spark DataFrame\n",
        "indexed_df = indexed_df.withColumn(\"prediction\", predict_udf(*[indexed_df[col+\"_Index\"] for col in feature_cols]))\n",
        "\n",
        "# Evaluation (accuracy)\n",
        "accuracy = indexed_df.filter(\n",
        "    pyspark_col(\"treatment\").isNotNull() & pyspark_col(\"prediction\").isNotNull()\n",
        ").withColumn(\n",
        "    \"correct\", when(pyspark_col(\"label\") == pyspark_col(\"prediction\").cast(\"double\"), 1).otherwise(0)\n",
        ").selectExpr(\"avg(correct) as accuracy\").first()[\"accuracy\"]\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Show predictions\n",
        "indexed_df.select(\"treatment\", \"prediction\").show(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Duk6vqpipD5b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ythkFZSOpD07"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaqguCSRGmN_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
